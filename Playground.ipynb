{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1298a36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core utilities + data handling\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import zipfile\n",
    "from io import BytesIO\n",
    "import ast\n",
    "from collections import Counter\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "# Downloading / I/O\n",
    "import requests\n",
    "import gdown\n",
    "\n",
    "# Models / embeddings\n",
    "import clip\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "\n",
    "# Image preprocessing for visualization/model input\n",
    "size_ = 512\n",
    "resize = transforms.Resize(size_)\n",
    "center_crop = transforms.CenterCrop(size_)\n",
    "\n",
    "# Use GPU if available, otherwise fall back to CPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa79fe0e",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34017685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset from the following link (skip if you already have the dataset):\n",
    "file_id = '1KJot5VeUSCUg11IuDYqmlS024MTqhUnQ'\n",
    "url = f'https://drive.google.com/uc?id={file_id}'\n",
    "\n",
    "output = 'Visual-Interestingness-Decoded-Dataset.zip'\n",
    "\n",
    "gdown.download(url, output, quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271a9302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the ZIP file from the in-memory buffer in read mode\n",
    "with zipfile.ZipFile(output, \"r\") as z:\n",
    "    # Extract all files from the ZIP into the current directory\n",
    "    z.extractall(Path(\"./\"))\n",
    "\n",
    "# Recursively find all CSV files in the extracted content\n",
    "csv_files = sorted(Path(\"./\").rglob(\"*.csv\"))\n",
    "\n",
    "# Print how many CSV files were found\n",
    "print(\"CSVs found:\", len(csv_files))\n",
    "\n",
    "# Print the paths of the CSV files (for inspection)\n",
    "for p in csv_files:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c24040b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_images = pd.read_csv(csv_files[0])\n",
    "df_relative_int = pd.read_csv(csv_files[1])\n",
    "df_single_int = pd.read_csv(csv_files[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38a0287",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_images.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60268ab5",
   "metadata": {},
   "source": [
    "#### Example Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a7cc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a specific image and retrieve its metadata\n",
    "test_ID = 51654698266\n",
    "row = df_images[df_images[\"ImageID\"] == test_ID].iloc[0]\n",
    "\n",
    "print(\"Image Description:\", row.ImageDescription)\n",
    "\n",
    "# Download and load the image from its URL\n",
    "response = requests.get(row.ImageURL)\n",
    "url_img = Image.open(BytesIO(response.content)).convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47377fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display image\n",
    "fig, axs = plt.subplots(1, 1, figsize=(5, 5))\n",
    "axs.imshow(center_crop(resize(url_img)))\n",
    "axs.set_title('URL Image')\n",
    "axs.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce590a8f",
   "metadata": {},
   "source": [
    "#### Single Image Interestingness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19540d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the row corresponding to the given ImageID\n",
    "row = df_single_int[df_single_int[\"ImageID\"] == test_ID]\n",
    "\n",
    "# Utility to ensure values are proper Python lists\n",
    "# (handles Series, already-parsed lists, and stringified lists)\n",
    "def to_list(x):\n",
    "    if isinstance(x, pd.Series):\n",
    "        x = x.iloc[0]\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    if isinstance(x, str):\n",
    "        return ast.literal_eval(x)\n",
    "    return [x]\n",
    "\n",
    "# Pretty-print paired answers and explanations for one source\n",
    "def pretty_print(name, answers, explanations):\n",
    "    answers = to_list(answers)\n",
    "    explanations = to_list(explanations)\n",
    "\n",
    "    print(f\"\\n{name}:\")\n",
    "    for a, e in zip(answers, explanations):\n",
    "        print(f\"  {a} - '{e}'\")\n",
    "\n",
    "# Display annotations for different annotators / models\n",
    "pretty_print(\"Human\", row[\"Human_Answers\"], row[\"Human_Explanations\"])\n",
    "pretty_print(\"GPT-4o\", row[\"GPT_Answers\"], row[\"GPT_Explanations\"])\n",
    "pretty_print(\"DeepSeek-VL2\", row[\"DeepSeek_Answers\"], row[\"DeepSeek_Explanations\"])\n",
    "pretty_print(\"Llama 3.2\", row[\"Llama_Answers\"], row[\"Llama_Explanations\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81214b9d",
   "metadata": {},
   "source": [
    "#### Relative Image Interestingness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303ca55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute a majority decision (\"first\" or \"second\") from a list of answers\n",
    "def majority_vote(answers):\n",
    "    answers = to_list(answers)          # Ensure input is a proper list\n",
    "    if len(answers) == 0:\n",
    "        return None\n",
    "\n",
    "    # Count how often each answer occurs\n",
    "    counts = Counter(answers)\n",
    "\n",
    "    # Ignore any labels other than \"first\" and \"second\"\n",
    "    counts = {k: v for k, v in counts.items() if k in [\"first\", \"second\"]}\n",
    "    if len(counts) == 0:\n",
    "        return None\n",
    "\n",
    "    # Return the label with the highest count\n",
    "    most_common = Counter(counts).most_common()\n",
    "    return most_common[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1a248e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all relative-annotation rows where the test image appears (as ImageID1 or ImageID2)\n",
    "rows = df_relative_int[\n",
    "    (df_relative_int[\"ImageID1\"] == test_ID) | \n",
    "    (df_relative_int[\"ImageID2\"] == test_ID)\n",
    "]\n",
    "\n",
    "# Attach image URLs for both images in each pair\n",
    "rows = (\n",
    "    rows\n",
    "    .merge(\n",
    "        df_images[[\"ImageID\", \"ImageURL\"]],\n",
    "        left_on=\"ImageID1\",\n",
    "        right_on=\"ImageID\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "    .rename(columns={\"ImageURL\": \"ImageURL1\"})\n",
    "    .drop(columns=[\"ImageID\"])\n",
    ")\n",
    "\n",
    "rows = (\n",
    "    rows\n",
    "    .merge(\n",
    "        df_images[[\"ImageID\", \"ImageURL\"]],\n",
    "        left_on=\"ImageID2\",\n",
    "        right_on=\"ImageID\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "    .rename(columns={\"ImageURL\": \"ImageURL2\"})\n",
    "    .drop(columns=[\"ImageID\"])\n",
    ")\n",
    "\n",
    "# Compute majority preference (\"first\" / \"second\") for humans and GPT\n",
    "rows[\"Human_Majority\"] = rows[\"Human_Answers\"].apply(majority_vote)\n",
    "rows[\"GPT_Majority\"]   = rows[\"GPT_Answers\"].apply(majority_vote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedecebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a grid of subplots to show image pairs\n",
    "fig, ax = plt.subplots(5, 2, figsize=(5, 12))\n",
    "\n",
    "for i in range(len(rows)):\n",
    "    # Load, resize, and center-crop both images in the pair\n",
    "    img1 = center_crop(resize(Image.open(BytesIO(requests.get(rows.iloc[i][\"ImageURL1\"]).content))))\n",
    "    img2 = center_crop(resize(Image.open(BytesIO(requests.get(rows.iloc[i][\"ImageURL2\"]).content))))\n",
    "\n",
    "    # Display the image pair\n",
    "    ax[i, 0].imshow(img1, cmap=\"gray\")\n",
    "    ax[i, 0].axis(\"off\")\n",
    "    ax[i, 1].imshow(img2, cmap=\"gray\")\n",
    "    ax[i, 1].axis(\"off\")\n",
    "\n",
    "    # Highlight the image preferred by the human majority\n",
    "    if rows.iloc[i][\"Human_Majority\"] == \"first\":\n",
    "        rect = mpatches.Rectangle((0, 0), size_, size_, linewidth=10,\n",
    "                                  edgecolor=\"g\", facecolor=\"none\")\n",
    "        ax[i, 0].add_patch(rect)\n",
    "    elif rows.iloc[i][\"Human_Majority\"] == \"second\":\n",
    "        rect = mpatches.Rectangle((0, 0), size_, size_, linewidth=10,\n",
    "                                  edgecolor=\"g\", facecolor=\"none\")\n",
    "        ax[i, 1].add_patch(rect)\n",
    "\n",
    "    # Print majority decisions for this image pair\n",
    "    print(\n",
    "        f\"Image {i+1}:\\n\"\n",
    "        f\"Human Majority: {rows.iloc[i]['Human_Majority']}\\n\"\n",
    "        f\"GPT Majority: {rows.iloc[i]['GPT_Majority']}\\n\"\n",
    "    )\n",
    "\n",
    "# Adjust layout and render the figure\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8ba72a",
   "metadata": {},
   "source": [
    "#### Computational Model (Relative Interestingness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54d86cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CLIP ViT-L/14 model and its preprocessing pipeline\n",
    "clip_model, preprocess = clip.load(\"ViT-L/14\", device=device)\n",
    "\n",
    "# Define the network for predicting relative interestingness\n",
    "class BaseNetwork(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(BaseNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 1)  # single-score output\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc1(x)\n",
    "\n",
    "# Initialize the model, load pretrained weights, and set it to evaluation mode\n",
    "RI_model = BaseNetwork(input_dim=768).to(device)\n",
    "RI_model.load_state_dict(torch.load(\"RI_model.pth\"))\n",
    "RI_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aab5532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all image URLs from both columns and remove duplicates\n",
    "image_urls = rows[\"ImageURL1\"].tolist() + rows[\"ImageURL2\"].tolist()\n",
    "image_urls = list(set(image_urls))\n",
    "\n",
    "# Download, resize, and center-crop all unique images\n",
    "images = [\n",
    "    center_crop(\n",
    "        resize(Image.open(BytesIO(requests.get(url).content)).convert(\"RGB\"))\n",
    "    )\n",
    "    for url in image_urls\n",
    "]\n",
    "\n",
    "# Extract CLIP image embeddings\n",
    "image_embeddings = []\n",
    "with torch.no_grad():\n",
    "    for img in images:\n",
    "        img_input = preprocess(img).unsqueeze(0).to(device)\n",
    "        embedding = clip_model.encode_image(img_input)\n",
    "        image_embeddings.append(embedding.cpu())\n",
    "\n",
    "# Predict relative-interestingness scores for each image\n",
    "ri_scores = []\n",
    "with torch.no_grad():\n",
    "    for emb in image_embeddings:\n",
    "        emb = emb.to(device).float()\n",
    "        score = RI_model(emb).item()\n",
    "        ri_scores.append(score)\n",
    "\n",
    "# Create lookup tables from image URLs to scores and loaded images\n",
    "url_to_ri_score = {url: score for url, score in zip(image_urls, ri_scores)}\n",
    "url_to_image = {url: img for url, img in zip(image_urls, images)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674a7ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort images by predicted RI score (highest first)\n",
    "sorted_images = sorted(\n",
    "    url_to_ri_score.items(), key=lambda x: x[1], reverse=True\n",
    ")\n",
    "\n",
    "# Visualize the top-6 most interesting images\n",
    "fig, axs = plt.subplots(1, 6, figsize=(18, 5))\n",
    "for i in range(6):\n",
    "    url, score = sorted_images[i]\n",
    "    img = url_to_image[url]\n",
    "\n",
    "    axs[i].imshow(img)\n",
    "    axs[i].set_title(f\"RI Score: {score:.4f}\")\n",
    "    axs[i].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envVis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
